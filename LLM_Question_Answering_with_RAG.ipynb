{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Experiment Using LLMs for Question Answering with Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Overview\n",
    "This notebook implements an NLP experiment using a Large Language Model (LLM) to answer questions based on different configurations, including retrieval-augmented generation (RAG). It employs both a plain LLM chain and RAG-based methods to enhance the model's ability to answer questions with external data.\n",
    "\n",
    "### Key Features:\n",
    "- **Plain LLM Chain:** Implements a basic LLM chain using a Hugging Face model with prompt templates.\n",
    "- **Retrieval Augmented Generation (RAG):** \n",
    "  - Retrieves information from an external file (`data/cats_content.txt`) to support answering questions.\n",
    "  - Uses the FAISS vector database for document retrieval based on embeddings generated by a sentence-transformer model.\n",
    "- **Multiple Experiment Methods:**\n",
    "  - Plain LLM response without external data.\n",
    "  - RAG chain with source tracking using `RetrievalQA`.\n",
    "  - Experimental method using `RetrievalQAWithSourcesChain` to provide answers with sources.\n",
    "\n",
    "### Implemented Components:\n",
    "- **Hugging Face Hub Integration:** Uses a T5 model (`google/flan-t5-large`) for the LLM tasks.\n",
    "- **Embedding Model:** Utilizes `sentence-transformers/all-MiniLM-L6-v2` to generate embeddings for document retrieval.\n",
    "- **Prompt Templates:** Customizable prompt templates for both plain LLM and RAG setups.\n",
    "- **FAISS Vector Store:** Efficient vector-based retrieval for documents to support the RAG chain.\n",
    "- **Chain Setup Functions:** Includes functions to initialize and configure the plain LLM, RAG, and RAG with sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_hW-qjooIi7"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.25.0 bertopic==0.15.0 datasets==2.14.4\n",
    "!pip install -q faiss-cpu==1.7.4 langchain==0.0.348 langchainhub==0.1.14\n",
    "!pip install -q sentence-transformers==2.2.2 sentencepiece==0.1.99 transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wMidFk7H7jI"
   },
   "outputs": [],
   "source": [
    "%env HUGGINGFACEHUB_API_TOKEN=<YOUR_API_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjledESTotU1"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from langchain import HuggingFaceHub, hub\n",
    "from langchain.chains import LLMChain, RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-PKfFwJowxI"
   },
   "outputs": [],
   "source": [
    "def get_logger(name: str = __name__) -> logging.Logger:\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s:%(module)s:%(funcName)s:%(levelname)s: %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18228,
     "status": "ok",
     "timestamp": 1702739164750,
     "user": {
      "displayName": "Даниил Богданов",
      "userId": "04866225263346705520"
     },
     "user_tz": -180
    },
    "id": "NPf8a5aapJBt",
    "outputId": "17e35d5a-8020-484d-d573-ba76e45720dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "INFO:__main__:Initializing embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Loading documents from file: data/cats_content.txt\n",
      "INFO:__main__:Creating FAISS vector database\n",
      "INFO:__main__:Setting up plain LLM chain\n",
      "INFO:__main__:Downloading prompt template: rlm/rag-prompt\n",
      "INFO:__main__:Setting up RAG LLM chain\n",
      "INFO:__main__:Downloading prompt template: rlm/rag-prompt\n",
      "INFO:__main__:Setting up simple RAG LLM chain with sources\n",
      "INFO:__main__:Setting up experimental RAG LLM chain with sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting answering questions! \n",
      "\n",
      "\n",
      "'Are cats good jumpers?' - expected answer: yes \n",
      "\n",
      "\tPlain llm answer: no \n",
      "\n",
      "\tRAG llm answer: yes \n",
      "\n",
      "\tRAG with sources llm answers:\n",
      "\n",
      "\t\tDumb method:\n",
      "\t\t\tAnswer: yes\n",
      "\t\t\tSources: [Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats are nearsighted, but their peripheral vision and night vision are much better than that of', metadata={'source': 'data/cats_content.txt', 'start_index': 371}), Document(page_content='Cats’ collarbones don’t connect to their other bones, as these bones are buried in their shoulder', metadata={'source': 'data/cats_content.txt', 'start_index': 744})]\n",
      "\n",
      "\t\tSimple method:\n",
      "\t\t\tAnswer: yes\n",
      "\t\t\tSources: [Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats are nearsighted, but their peripheral vision and night vision are much better than that of', metadata={'source': 'data/cats_content.txt', 'start_index': 371}), Document(page_content='Cats’ collarbones don’t connect to their other bones, as these bones are buried in their shoulder', metadata={'source': 'data/cats_content.txt', 'start_index': 744})]\n",
      "\n",
      "\t\tExperimental method:\n",
      "\t\t\tAnswer: \n",
      "\t\t\tSources: Cats can jump up to six times their length.\n",
      "\n",
      "\n",
      "'How far can cat jump in compare with its own length?' - expected answer: up to 6 times \n",
      "\n",
      "\tPlain llm answer: a few feet \n",
      "\n",
      "\tRAG llm answer: six times their length \n",
      "\n",
      "\tRAG with sources llm answers:\n",
      "\n",
      "\t\tDumb method:\n",
      "\t\t\tAnswer: six times their length\n",
      "\t\t\tSources: [Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475})]\n",
      "\n",
      "\t\tSimple method:\n",
      "\t\t\tAnswer: six times their length\n",
      "\t\t\tSources: [Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475})]\n",
      "\n",
      "\t\tExperimental method:\n",
      "\t\t\tAnswer: \n",
      "\t\t\tSources: Cats can jump up to six times their length.\n",
      "\n",
      "\n",
      "'How many bones does cat have?' - expected answer: 230 \n",
      "\n",
      "\tPlain llm answer: ten \n",
      "\n",
      "\tRAG llm answer: 230 \n",
      "\n",
      "\tRAG with sources llm answers:\n",
      "\n",
      "\t\tDumb method:\n",
      "\t\t\tAnswer: 230\n",
      "\t\t\tSources: [Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475}), Document(page_content='Cats’ collarbones don’t connect to their other bones, as these bones are buried in their shoulder', metadata={'source': 'data/cats_content.txt', 'start_index': 744}), Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568})]\n",
      "\n",
      "\t\tSimple method:\n",
      "\t\t\tAnswer: 230\n",
      "\t\t\tSources: [Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475}), Document(page_content='Cats’ collarbones don’t connect to their other bones, as these bones are buried in their shoulder', metadata={'source': 'data/cats_content.txt', 'start_index': 744}), Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568})]\n",
      "\n",
      "\t\tExperimental method:\n",
      "\t\t\tAnswer: \n",
      "\t\t\tSources: Cats have 230 bones, while humans only have 206.\n",
      "\n",
      "\n",
      "'How many toes does each front paw of a cat has?' - expected answer: five \n",
      "\n",
      "\tPlain llm answer: four \n",
      "\n",
      "\tRAG llm answer: five toes \n",
      "\n",
      "\tRAG with sources llm answers:\n",
      "\n",
      "\t\tDumb method:\n",
      "\t\t\tAnswer: five toes\n",
      "\t\t\tSources: [Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475}), Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568})]\n",
      "\n",
      "\t\tSimple method:\n",
      "\t\t\tAnswer: five toes\n",
      "\t\t\tSources: [Document(page_content='Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).', metadata={'source': 'data/cats_content.txt', 'start_index': 475}), Document(page_content='Cats have 230 bones, while humans only have 206.', metadata={'source': 'data/cats_content.txt', 'start_index': 851}), Document(page_content='Cats’ claws all curve downward, which means that they can’t climb down trees head-first. Instead,', metadata={'source': 'data/cats_content.txt', 'start_index': 612}), Document(page_content='Cats can jump up to six times their length.', metadata={'source': 'data/cats_content.txt', 'start_index': 568})]\n",
      "\n",
      "\t\tExperimental method:\n",
      "\t\t\tAnswer: \n",
      "\t\t\tSources: Cats are supposed to have 18 toes (five toes on each front paw; four toes on each back paw).\n"
     ]
    }
   ],
   "source": [
    "class QuestionAnsweringExperiment:\n",
    "    \"\"\"\n",
    "    This class defines logic to setup an experiment to test how model can answer questions with:\n",
    "    - plain llm chain. Just an llm and a prompt using langchain.\n",
    "    - Retrieval augmeneted generation chain - a chain with the access to the external file content.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        repo_id: str,\n",
    "        external_data_path: str,\n",
    "        embedding_model_name: str,\n",
    "        plain_lmm_prompt_template: str,\n",
    "        rag_llm_prompt_template_name: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the experiment setup for testing models in answering questions using both plain LLM chain and Retrieval Augmented Generation (RAG) chain\n",
    "\n",
    "        :param repo_id: The repository ID for the Hugging Face model\n",
    "        :param external_data_path: Path to the external data file to be used for RAG\n",
    "        :param embedding_model_name: Name of the model for generating embeddings\n",
    "        :param plain_lmm_prompt_template: Prompt template for the plain LLM model\n",
    "        :param rag_llm_prompt_template_name: The repository ID for the Hugging Face prompt template\n",
    "        \"\"\"\n",
    "        self.llm = HuggingFaceHub(\n",
    "            repo_id=repo_id, model_kwargs={\"temperature\": 0.05, \"max_length\": 200}\n",
    "        )\n",
    "\n",
    "        self.embedding_model = None\n",
    "        self.vectorstore = None\n",
    "\n",
    "        self.init_embedding_model(embedding_model_name)\n",
    "\n",
    "        self.create_vector_database(\n",
    "            self.load_documents_from_file(\n",
    "                external_data_path, chunk_size=100, chunk_overlap=10\n",
    "            ),\n",
    "            self.embedding_model,\n",
    "        )\n",
    "\n",
    "        self.setup_plain_llm_chain(plain_lmm_prompt_template)\n",
    "\n",
    "        self.setup_rag_llm_chain(\n",
    "            rag_llm_prompt_template_name\n",
    "        )\n",
    "\n",
    "        self.setup_rag_with_source_llm_chain(\n",
    "             rag_llm_prompt_template_name\n",
    "        )\n",
    "\n",
    "    def setup_plain_llm_chain(self, prompt_template: str):\n",
    "        \"\"\"\n",
    "        Sets up the plain LLM (Language Model) chain by initializing an LLMChain with the provided LLM and prompt template\n",
    "\n",
    "        :param prompt_template: The prompt template to use with the plain LLM chain\n",
    "        \"\"\"\n",
    "        logger.info(\"Setting up plain LLM chain\")\n",
    "        self.plain_llm_chain = LLMChain(\n",
    "            llm=self.llm, prompt=PromptTemplate.from_template(prompt_template)\n",
    "        )\n",
    "\n",
    "    def init_embedding_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the embedding model from the Hugging Face Hub using the specified model name\n",
    "\n",
    "        :param model_name: Name of the model to be used for generating embeddings\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing embedding model: {model_name}\")\n",
    "        if self.embedding_model is None:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        else:\n",
    "            logger.info(\"Embedding model has been already initialized\")\n",
    "\n",
    "    def create_vector_database(self, documents: list, embedding_model):\n",
    "        \"\"\"\n",
    "        Creates a FAISS vector database, adding documents and their corresponding embeddings generated using the provided embedding model\n",
    "\n",
    "        :param documents: List of documents to be added to the database\n",
    "        :param embedding_model: The embedding model used to generate embeddings for the documents\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating FAISS vector database\")\n",
    "        self.vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    def load_documents_from_file(\n",
    "        self, file_path: str, chunk_size: int = 100, chunk_overlap: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads documents from a specified file path, splitting the text into chunks if necessary\n",
    "\n",
    "        :param file_path: Path to the file containing the documents\n",
    "        :param chunk_size: The size of each text chunk (default is 100)\n",
    "        :param chunk_overlap: The overlap between chunks (default is 10)\n",
    "        :return: A list of documents splitted into chunks\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading documents from file: {file_path}\")\n",
    "        loader = TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "        )\n",
    "        docs_splitted = text_splitter.split_documents(documents)\n",
    "        return docs_splitted\n",
    "\n",
    "    def download_prompt_from_hub(self, prompt_name):\n",
    "        \"\"\"\n",
    "        Downloads a prompt template by its name from the Langchain Hub.\n",
    "\n",
    "        :param prompt_name: The name (Hugging Face repo ID) of the prompt template to be downloaded\n",
    "        :return: The downloaded prompt template\n",
    "        \"\"\"\n",
    "        logger.info(f\"Downloading prompt template: {prompt_name}\")\n",
    "        prompt_template = hub.pull(prompt_name)\n",
    "        return prompt_template\n",
    "\n",
    "    def setup_rag_llm_chain(\n",
    "        self, rag_llm_prompt_template_name\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up the Retrieval Augmented Generation (RAG) LLM chain, including loading documents, initializing the embedding model, creating a FAISS database, and initializing the RAG chain\n",
    "\n",
    "        :param rag_llm_prompt_template_name: Name (Hugging Face repo ID) of the RAG LLM prompt template\n",
    "        \"\"\"\n",
    "        prompt_template = self.download_prompt_from_hub(rag_llm_prompt_template_name)\n",
    "\n",
    "        logger.info(\"Setting up RAG LLM chain\")\n",
    "        self.rag_llm_chain = RetrievalQA.from_llm(\n",
    "            llm=self.llm,\n",
    "            prompt=prompt_template,\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "        )\n",
    "\n",
    "    def setup_rag_with_source_llm_chain(\n",
    "        self, rag_llm_prompt_template_name\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes and sets up the Retrieval Augmented Generation (RAG) LLM chain with source document tracking\n",
    "\n",
    "        :param rag_llm_prompt_template_name: Name (Hugging Face repo ID) of the RAG LLM prompt template\n",
    "        \"\"\"\n",
    "        prompt_template = self.download_prompt_from_hub(rag_llm_prompt_template_name)\n",
    "\n",
    "        logger.info(\"Setting up simple RAG LLM chain with sources\")\n",
    "        self.simple_rag_with_source_llm_chain = RetrievalQA.from_llm(\n",
    "            llm=self.llm,\n",
    "            prompt=prompt_template,\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Setting up experimental RAG LLM chain with sources\")\n",
    "        self.rag_with_source_llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "        )\n",
    "\n",
    "    def predict_with_llm_chain(self, query):\n",
    "        \"\"\"\n",
    "        Generates a prediction for a given query using the plain LLM chain\n",
    "\n",
    "        :param query: The query string to be answered\n",
    "        :return: The generated answer from the plain LLM chain\n",
    "        :raises AttributeError: If the plain_llm_chain attribute does not exist\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"plain_llm_chain\"):\n",
    "            raise AttributeError(\"Please set up a chain before calling predict\")\n",
    "        return self.plain_llm_chain.run(query)\n",
    "\n",
    "    def predict_with_rag_chain(self, query):\n",
    "        \"\"\"\n",
    "        Generates a prediction for a given query using the RAG LLM chain\n",
    "\n",
    "        :param query: The query string to be answered\n",
    "        :return: The generated answer from the RAG LLM chain\n",
    "        :raises AttributeError: If the rag_llm_chain attribute does not exist\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"rag_llm_chain\"):\n",
    "            raise AttributeError(\n",
    "                \"RAG LLM chain is not set up. Please set it up before calling predict\"\n",
    "            )\n",
    "\n",
    "        response = self.rag_llm_chain.run(query)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def predict_rag_with_source_chain(self, query):\n",
    "        \"\"\"\n",
    "        Generates a prediction for a given query using the RAG LLM chain that also returns the source document information\n",
    "\n",
    "        :param query: The query string to be answered\n",
    "        :return: The generated answer from the RAG LLM chain with different methods\n",
    "        :raises AttributeError: If the rag_llm_chain attribute does not exist\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Manually getting the docs\n",
    "        retrieved_docs = self.rag_llm_chain.retriever.invoke(query)\n",
    "        response = self.rag_llm_chain.run(query)\n",
    "        results[\"dumb\"] = {\"answer\": response, \"sources\": retrieved_docs}\n",
    "\n",
    "        # The same method but from model (using return_source_documents=True)\n",
    "        response = self.simple_rag_with_source_llm_chain(query)\n",
    "        results[\"simple\"] = {\n",
    "            \"answer\": response[\"result\"],\n",
    "            \"sources\": response[\"source_documents\"],\n",
    "        }\n",
    "\n",
    "        # Trying RetrievalQAWithSourcesChain\n",
    "        response = self.rag_with_source_llm_chain(query)\n",
    "        results[\"experimental\"] = {\n",
    "            \"answer\": response[\"answer\"],\n",
    "            \"sources\": response[\"sources\"],\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main entrypoint.\n",
    "    1) Initialises experiment as an instance of QuestionAnsweringExperiment with passed params.\n",
    "    2) initialises a set of test queries.\n",
    "    3) iterate queries and runs \"predict_with_llm_chain\" and \"predict_with_rag_chain\" methods.\n",
    "    4) prints results of generated answers for both setups as well as expected result.\n",
    "    \"\"\"\n",
    "\n",
    "    qa_prompt_template = \"\"\"Answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    experiment = QuestionAnsweringExperiment(\n",
    "        repo_id=\"google/flan-t5-large\",\n",
    "        external_data_path=\"data/cats_content.txt\",\n",
    "        embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        plain_lmm_prompt_template=qa_prompt_template,\n",
    "        rag_llm_prompt_template_name=\"rlm/rag-prompt\",\n",
    "    )\n",
    "\n",
    "    queries = [\n",
    "        (\"Are cats good jumpers?\", \"yes\"),\n",
    "        (\"How far can cat jump in compare with its own length?\", \"up to 6 times\"),\n",
    "        (\"How many bones does cat have?\", \"230\"),\n",
    "        (\"How many toes does each front paw of a cat has?\", \"five\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\nStarting answering questions! \\n\")\n",
    "    for query, answer in queries:\n",
    "        print(f\"\\n'{query}' - expected answer: {answer} \\n\")\n",
    "        plain_llm_answer = experiment.predict_with_llm_chain(query)\n",
    "        print(f\"\\tPlain llm answer: {plain_llm_answer} \\n\")\n",
    "        rag_llm_answer = experiment.predict_with_rag_chain(query)\n",
    "        print(f\"\\tRAG llm answer: {rag_llm_answer} \\n\")\n",
    "        print(\"\\tRAG with sources llm answers:\\n\")\n",
    "\n",
    "        results = experiment.predict_rag_with_source_chain(query)\n",
    "        for key in results.keys():\n",
    "            print(f\"\\t\\t{key.capitalize()} method:\")\n",
    "            print(f\"\\t\\t\\tAnswer: {results[key]['answer']}\")\n",
    "            print(f\"\\t\\t\\tSources: {results[key]['sources']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdlL70m1Kf9K"
   },
   "source": [
    "Unfortunately, I ran into an unknown problem of lack of answer from the `RetrievalQAWithSourcesChain` model :(  \n",
    "But in returns correct source"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEAUyPryQxYIyWuc0yS/Ge",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
